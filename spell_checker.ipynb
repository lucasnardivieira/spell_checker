{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separete_words(tokens): #this function is responsable to get only words from the token list [\"word\", \"?\", \"word\", \".\"]\n",
    "  word_list = []\n",
    "  for token in tokens:\n",
    "    if token.isalpha():\n",
    "      word_list.append(token)\n",
    "    else:\n",
    "      continue\n",
    "  return word_list\n",
    "\n",
    "def normalize_words(list): #this function is responsable to set all characters in lower case\n",
    "  word_list = []\n",
    "  for word in list:\n",
    "    word_list.append(word.lower())\n",
    "  return word_list\n",
    "\n",
    "def generate_words(word): #this function is responsable to genarate all possible words for all possible mistakes\n",
    "  slices = []\n",
    "  for i in range(len(word) + 1):\n",
    "    slices.append((word[:i], word[i:]))\n",
    "  print(\"fatias geradas\")\n",
    "  new_words = insert_letter(slices)\n",
    "  new_words += delete_character(slices)\n",
    "  new_words += change_character(slices)\n",
    "  new_words += change_pos(slices)\n",
    "  return new_words\n",
    "\n",
    "# possible mistake (case 1): word misses a letter\n",
    "def insert_letter(slices):\n",
    "  new_words = []\n",
    "  letters = \"abcdefghijklmnopqrstuvwxyzáàãâéèêíìîóòõôúùûç\"\n",
    "  for l_slice, r_slice in slices:\n",
    "    for letter in letters:\n",
    "      new_words.append( l_slice + letter + r_slice)\n",
    "  print(\"caso 1 feito\")\n",
    "  return new_words\n",
    "\n",
    "# possible mistake (case 2): word has an additional letter\n",
    "def delete_character(slices):\n",
    "    new_words = []\n",
    "    for l_slice, r_slice in slices:\n",
    "        new_words.append(l_slice + r_slice[1:])\n",
    "    print(\"caso 2 feito\")\n",
    "    return new_words\n",
    "\n",
    "# possible mistake (case 3): word has a wrong letter\n",
    "def change_character(slices):\n",
    "    new_words = []\n",
    "    letters = \"abcdefghijklmnopqrstuvwxyzáàãâéèêíìîóòõôúùûç\"\n",
    "    for l_slice, r_slice in slices:\n",
    "        for letter in letters:\n",
    "            new_words.append(l_slice + letter + r_slice[1:])\n",
    "    print(\"caso 3 feito\")\n",
    "    return new_words\n",
    "\n",
    "# possible mistake (case 4): word has letter switched place with the next one\n",
    "def change_pos(slices):\n",
    "    new_words = []\n",
    "    for l_slice, r_slice in slices:\n",
    "        if len(r_slice) > 1:\n",
    "            new_words.append(l_slice + r_slice[1] + r_slice[0] + r_slice[2:])\n",
    "    print(\"caso 4 feito\")\n",
    "    return new_words\n",
    "\n",
    "def probability(generated_word):\n",
    "    print(\"calculando a probabilidade da palvra\")\n",
    "    freq = nltk.FreqDist(word_list)\n",
    "    word_count = len(word_list)\n",
    "    return freq[generated_word]/word_count\n",
    "\n",
    "def create_test_data(file):\n",
    "  test_words = []\n",
    "  f = open(file, \"r\", encoding=\"utf8\")\n",
    "  for line in f:\n",
    "    right_word, wrong_word = line.split()\n",
    "    test_words.append((right_word, wrong_word))\n",
    "  f.close()\n",
    "  return test_words\n",
    "\n",
    "def validate_corrector(test_words, vocabulary):\n",
    "    number_test_words = len(test_words)\n",
    "    got_right = 0\n",
    "    unknow_words = 0\n",
    "    for right_word, wrong_word in test_words:\n",
    "        print(\"validando corretor\")\n",
    "        corrected_word = corrector(wrong_word)\n",
    "        if corrected_word == right_word:\n",
    "            got_right += 1\n",
    "        else:\n",
    "          unknow_words += (right_word not in vocabulary)\n",
    "    hit_rate = round(got_right * 100/number_test_words, 2)\n",
    "    unknow_rate = round(unknow_words*100/number_test_words, 2)\n",
    "    print(f'{hit_rate}% de palavras corrigidas a partir de {number_test_words} palavras, desconhecida é {unknow_rate}%')\n",
    "\n",
    "\n",
    "def corrector(word):\n",
    "    generated_words = generate_words(word)\n",
    "    right_word = max(generated_words, key = probability)\n",
    "    print(right_word)\n",
    "    return right_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"artigos.txt\", \"r\", encoding=\"utf8\") as file:\n",
    "    artigos = file.read()\n",
    "\n",
    "tokens = nltk.tokenize.word_tokenize(artigos)\n",
    "word_list = separete_words(tokens)\n",
    "word_list = normalize_words(word_list)\n",
    "test_words = create_test_data(\"palavras.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_corrector(test_words, set(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5ff168696553f3e2cb115c273c46eeb315c32b0d0536992c4bad2740a7f9469"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
