{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhEpA1N29-cz",
        "outputId": "80f94d90-87dd-471f-a10f-be4443d4c329"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\lucas\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QTSZ0LCbFdcQ"
      },
      "outputs": [],
      "source": [
        "def separate_words(tokens): #this function is responsable to get only words from the token list [\"word\", \"?\", \"word\", \".\"]\n",
        "  word_list = []\n",
        "  for token in tokens:\n",
        "    if token.isalpha():\n",
        "      word_list.append(token)\n",
        "    else:\n",
        "      continue\n",
        "  return word_list\n",
        "\n",
        "def normalize_words(list): #this function is responsable to set all characters in lower case\n",
        "  word_list = []\n",
        "  for word in list:\n",
        "    word_list.append(word.lower())\n",
        "  return word_list\n",
        "\n",
        "def generate_words(word): #this function is responsable to genarate all possible words for all possible mistakes\n",
        "  slices = []\n",
        "  for i in range(len(word) + 1):\n",
        "    slices.append((word[:i], word[i:]))\n",
        "  new_words = insert_letter(slices)\n",
        "  new_words += delete_character(slices)\n",
        "  new_words += change_character(slices)\n",
        "  new_words += change_pos(slices)\n",
        "  return new_words\n",
        "\n",
        "def generate_words_turbo(words): #this function is responsable to recall 'generate_words' and deal with mistaken 2 positions ahead\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "    new_words += generate_words(word)\n",
        "  return new_words\n",
        "\n",
        "def insert_letter(slices): # possible mistake (case 1): word misses a letter\n",
        "  new_words = []\n",
        "  letters = \"abcdefghijklmnopqrstuvwxyzáàãâéèêíìîóòõôúùûç\"\n",
        "  for l_slice, r_slice in slices:\n",
        "    for letter in letters:\n",
        "      new_words.append( l_slice + letter + r_slice)\n",
        "  return new_words\n",
        "\n",
        "def delete_character(slices): # possible mistake (case 2): word has an additional letter\n",
        "    new_words = []\n",
        "    for l_slice, r_slice in slices:\n",
        "        new_words.append(l_slice + r_slice[1:])\n",
        "    return new_words\n",
        "\n",
        "def change_character(slices): # possible mistake (case 3): word has a wrong letter\n",
        "    new_words = []\n",
        "    letters = \"abcdefghijklmnopqrstuvwxyzáàãâéèêíìîóòõôúùûç\"\n",
        "    for l_slice, r_slice in slices:\n",
        "        for letter in letters:\n",
        "            new_words.append(l_slice + letter + r_slice[1:])\n",
        "    return new_words\n",
        "\n",
        "def change_pos(slices): # possible mistake (case 4): word has letter switched place with the next one\n",
        "    new_words = []\n",
        "    for l_slice, r_slice in slices:\n",
        "        if len(r_slice) > 1:\n",
        "            new_words.append(l_slice + r_slice[1] + r_slice[0] + r_slice[2:])\n",
        "    return new_words\n",
        "\n",
        "def probability(generated_word): #this function is responsable to calculate the frequency a word is an amount of words\n",
        "    freq = nltk.FreqDist(word_list)\n",
        "    word_count = len(word_list)\n",
        "    return freq[generated_word]/word_count\n",
        "\n",
        "def create_test_data(file):\n",
        "  test_words = []\n",
        "  f = open(file, \"r\", encoding=\"utf8\")\n",
        "  for line in f:\n",
        "    right_word, wrong_word = line.split()\n",
        "    test_words.append((right_word, wrong_word))\n",
        "  f.close()\n",
        "  return test_words\n",
        "\n",
        "def validate_corrector(test_words, vocabulary):\n",
        "    number_test_words = len(test_words)\n",
        "    got_right = 0\n",
        "    unknow_words = 0\n",
        "    for right_word, wrong_word in test_words:\n",
        "        corrected_word = corrector(wrong_word)\n",
        "        unknow_words += (right_word not in vocabulary)\n",
        "        if corrected_word == right_word:\n",
        "            got_right += 1\n",
        "        else:\n",
        "          print(wrong_word + '-' + corrector(wrong_word) + '-' + new_corrector(wrong_word))\n",
        "    hit_rate = round(got_right * 100/number_test_words, 2)\n",
        "    unknow_rate = round(unknow_words*100/number_test_words, 2)\n",
        "    print(f'{hit_rate}% de palavras corrigidas a partir de {number_test_words} palavras, desconhecida é {unknow_rate}%')\n",
        "\n",
        "def corrector(word):\n",
        "    generated_words = generate_words(word)\n",
        "    right_word = max(generated_words, key = probability)\n",
        "    return right_word\n",
        "\n",
        "def new_corrector(word, list):\n",
        "    vocabulary = np.array(list)\n",
        "    print(type(vocabulary))\n",
        "    generated_words = generate_words(word)\n",
        "    turbo_words = generate_words_turbo(generated_words)\n",
        "    all_words = np.array(set(generated_words + turbo_words))\n",
        "    print(type(all_words))\n",
        "    print(all_words.size)\n",
        "    real_words = []\n",
        "    for word in np.turbo_words:\n",
        "        if word in vocabulary:\n",
        "          real_words.append(word)\n",
        "    print(len(real_words))\n",
        "    right_word = max(real_words, probability)\n",
        "    return right_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjKZTks5Hpr5"
      },
      "outputs": [],
      "source": [
        "with open(\"artigos.txt\", \"r\", encoding=\"utf8\") as file:\n",
        "    artigos = file.read()\n",
        "\n",
        "tokens = nltk.tokenize.word_tokenize(artigos)\n",
        "word_list = separete_words(tokens)\n",
        "word_list = normalize_words(word_list)\n",
        "test_words = create_test_data(\"palavras.txt\")\n",
        "vocabulary = word_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX1QWYmBnSmC",
        "outputId": "86edc614-3beb-4b0f-d365-07fceda8c09d"
      },
      "outputs": [],
      "source": [
        "words = generate_words_turbo(generate_words(\"lóiigica\"))\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPRWcrTju1sb",
        "outputId": "6d2944ac-639b-4d9f-bd84-94e4c1faa554"
      },
      "outputs": [],
      "source": [
        "validate_corrector(test_words, vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whZs-PAd0Uhv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "f5ff168696553f3e2cb115c273c46eeb315c32b0d0536992c4bad2740a7f9469"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
